% relation to grounded language learning tasks
Researchers have made substantial progress on grounding language for robots,
enabling tasks such as object recognition and route following from verbal
descriptions.  Early work used vision together with speech descriptions of
objects to learn grounded semantics~\cite{roy:cogsci02}.

In the past few years, much of this work has focused on combining language with
visual information.  For grounding referring expressions in an environment,
many learn perceptual classifiers for words such as `red', `square', and `left'
given some pairing of human descriptions and labeled
scenes~\cite{liu:acl14,malinowski:nips14,mohan:acs13,sun:icra13,dindo:iros10,vogel:aaai10}.
Some approaches additionally incorporate language models into the learning
phase~\cite{spranger:ijcai15,krishnamurthy:acl13,perera:aaai13,matuszek:icml12}.
Our method uses simple language understanding and constructs new attribute
classifiers for each unseen context word used by a human playing \ispy.
Outside of robotics, there has been some work on combining language with other
sensory modalities than vision, such as audio~\cite{kiela:emnlp15}.  Unlike
previous methods, our system is embodied in a learning robot that takes
advantage of more than just visual data from objects to ground language
predicates.

Including a human in the learning loop provides a more realistic learning
scenario for applications such as household and office robotics.  Past work has
used human speech plus gestures describing sets of objects on a table as
supervision to learn attribute classifiers~\cite{matuszek:aaai14,kollar:rss13}.
Recent work introduced the \ispy game as a supervisory framework for grounded
language learning~\cite{parde:ijcai15}.  Our work differs from these prior
projects in that we use additional sensory data beyond vision to build
object attribute classifiers.

In our instantiation of the \ispy task, the robot and the human both take a
turn describing objects, where in previous work~\cite{parde:ijcai15} only
the human played this role.  To our knowledge, ours is the first work to
integrate visual, haptic, auditory, and proprioceptive information for language
grounding by an embodied robot.

% relation to multi-modal perception (i.e. jivko's work)
% TODO: jivko
