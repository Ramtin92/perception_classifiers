To determine whether multi-modal perception helps a robot learn grounded language, we had two different systems play \ispy with 40 human subjects.
The baseline \textbf{vision only} system used only the \textbf{look} behavior when grounding language predicates, analogous to many past works as discussed in section~\ref{sec:relatedwork}.
Our \textbf{multi-modal} system used the full suite of behaviors and associated haptic, proprioceptive, and auditory modalities shown in Table~\ref{tab:feature_space_of_contexts} when grounding language predicates.