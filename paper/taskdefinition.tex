In our \ispy task\footnote{Video demonstrating the \ispy task and robot learning: \texttt{https://youtu.be/JdcmreKPC-k}}, the human and robot take turns describing objects from among 4 on a tabletop (Figure~\ref{fig:ispy}).
Participants were asked to describe objects using attributes.
As an example, we suggested participants describe an object as ``black rectangle'' as opposed to ``whiteboard eraser.''
Additionally, participants were told they could handle the objects physically before offering a description, but were not explicitly asked to use non-visual predicates.
Once participants offered a description, the robot guessed candidate objects in order of computed confidence (see Section~\ref{ssec:gll}) until one was confirmed correct.

In the second half of each round, the robot picked an object and then described it with up to three predicates (see Section~\ref{ssec:gll}).
The participant was again able to pick up and physically handle objects before guessing.
The robot confirmed or denied each participant guess until the correct object was chosen.

\ispy gameplay admits two metrics.
The \textbf{robot guess} metric is the number of turns the robot took to guess what object the participant was describing.
The \textbf{human guess} metric is the complement.
Using these metrics, we compare the performance of two \ispy playing systems (\textbf{multi-modal} and \textbf{vision-only}) as described in Section ~\ref{sec:experiment}.
We also compare the agreement between both systems' predicate classifiers and human labels acquired during the game.