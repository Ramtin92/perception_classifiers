We expand past work on grounding natural language in robot sensory perception by going beyond vision and exploiting haptic, auditory, and proprioceptive robot senses.
We compare a vision only grounding system to one that uses these additional senses by employing an embodied robot playing \ispy with many human users.
To our knowledge, this is the first robotic system to perform natural language grounding using multi-modal sensory perception through natural interaction with human users.

We demonstrate quantitatively, through the number of turns the robot needs to guess objects described by humans, as well as through agreement with humans on language predicate labels for objects, that our multi-modal framework learns more effective lexical groundings than using vision alone.
We also explore the systems' learned groundings qualitatively, showing words for which non-visual information helps most as well as when non-visual properties of objects correlate with learned meanings (e.g. ``empty'' negatively correlates with object weight).

In the future, we plan to explore better selection of predicates for describing objects in \ispy, improving grounded referring expression generation, as explored by~\cite{tellex:rss14}.
Additionally, we would like to detect polysemy for predicates whose meaning varies across sensory modalities.
For example, the word ``light'' can refer to weight or color; our systems' failure to distinguish these senses as users intermixed them resulted in poor understanding regardless of whether multi-modal perception was used.
