We expand past work on grounding natural language in robot sensory perception by going beyond vision and exploring haptic, auditory, and proprioceptive robot senses.
We compare a vision only grounding system to one that uses these additional senses by employing an embodied robot playing \ispy with many human users.
To our knowledge, ours is the first robotic system to perform natural language grounding using multi-modal sensory perception through natural interaction with human users.

We demonstrate quantitatively, through the number of turns the robot needs to guess objects described by humans, as well as through agreement with humans on language predicate labels for objects, that our multi-modal framework learns more effective lexical groundings than one using vision alone.
We also explore the learned groundings qualitatively, showing words for which non-visual information helps most as well as when non-visual properties of objects correlate with learned meanings (e.g. ``small'' correlates negatively with object weight).

In the future, we would like to use one-class classification methods~\cite{liu:icdm03} to remove the need for a follow-up dialog asking about particular predicates applied to an object to gather negative labels.
Additionally, we would like to detect polysemy for predicates whose meanings vary across sensory modalities.
For example, the word ``light'' can refer to weight or color.
Our current system fails to distinguish these senses, while human participants intermix them.
Additionally, in our current system, the robot needs to explore objects in advance using all of its behaviors.
However, for purely visual predicates like ``pink'' and other colors, only the {\it look} behavior is necessary to determine whether an object has the property.
We will work towards an exploration system that uses its learned knowledge of predicates from a game such as \ispy to determine the properties of a novel object while attempting to use as few exploratory behaviors as necessary.