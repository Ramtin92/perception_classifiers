We expand past work on grounding natural language in robot sensory perception by going beyond vision and into haptic, auditory, and proprioceptive robot senses.
We compare a vision only and multi-modal grounding system side-by-side on an embodied robot playing \ispy with many human users.
To our knowledge, this is the first robotic system to perform natural language grounding using multi-modal sensory perception through feedback with a human user alone.

We demonstrate quantitatively, through the number of turns the robot took to guess objects being described by humans as well as through system and human agreement on language predicate labels for objects, that grounding language in a multi-modal framework that goes beyond vision alone results in robot word understanding that more closely aligns with human understanding.
We also explore the systems' learned groundings qualitatively, showing words for which non-visual information helps most as well as when non-visual properties of objects correlate with learned meanings (e.g. ``empty'' negatively correlates with object weight).

In the future, we would like to explore better ways to select predicates with which to describe objects in the \ispy game, which falls into the space of grounded referring expression generation, as explored by~\cite{tellex:rss14}.
Additionally, we would like to explore polysemy detection for predicates that are polysemous across sensory modalities.
For example, the word ``light'' can mean light in weight or light in color; our systems' failure to distinguish these senses as users intermixed them resulted in poor understanding regardless of whether multi-modal perception was used.