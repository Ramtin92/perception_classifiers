We expand past work on grounding natural language in robot sensory perception by going beyond vision and exploring haptic, auditory, and proprioceptive robot senses.
We compare a vision only grounding system to one that uses these additional senses by employing an embodied robot playing \ispy with many human users.
To our knowledge, ours is the first robotic system to perform natural language grounding using multi-modal sensory perception through natural interaction with human users.

We demonstrate quantitatively, through the number of turns the robot needs to guess objects described by humans, as well as through agreement with humans on language predicate labels for objects, that our multi-modal framework learns more effective lexical groundings than one using vision alone.
We also explore the learned groundings qualitatively, showing words for which non-visual information helps most as well as when non-visual properties of objects correlate with learned meanings (e.g. ``empty'' inverse to object weight).

In the future, we would like to use one-class classification methods to remove the need for a follow-up dialog asking about particular predicates applied to an object to gather negative labels.
Additionally, we would like to detect polysemy for predicates whose meaning varies across sensory modalities.
For example, the word ``light'' can refer to weight or color.
Our current system fails to distinguish these senses, while human subjects intermix them.
