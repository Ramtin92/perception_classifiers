To determine whether our \textbf{multi-modal} approach outperformed a traditional \textbf{vision only} approach, we measured the average number of robot guesses and human guesses in games played with each fold of objects.
The systems were identical in fold 0 since both were untrained.
In the end, we trained the systems on all available data to calculate predicate classifier agreement with human labels.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/robot_guesses_error_bars.jpg}
\caption{Average expected number of guesses the robot made on each human turn with standard error bars shown.
\textbf{Bold}: significantly lower than the average at fold 0 with $p<0.05$ (unpaired Student's $t$-test).
\textbf{*}: significantly lower than the competing system on this fold on participant-by-participant basis with $p<0.05$ (paired Student's $t$-test).}
\label{fig:robot_guesses}
\end{figure}

\textbf{Robot guess.} Figure~\ref{fig:robot_guesses} shows the average number of robot guesses for the games in each fold. Because we had access to the scores the robot assigned each object, we calculated the {\it expected} number of robot guesses for each turn.
For example, if all 4 objects were tied for first, the expected number of robot guesses for that turn was 2.5, regardless of whether it got (un)lucky and picked the correct object (last)first.\footnote{2.5 is the expected number for 4 tied objects because the probability of picking in any order is equal, so the expected turn to get the correct object is $\frac{1+2+3+4}{4} = \frac{10}{4} = 2.5$}

After training on just one fold, our \textbf{multi-modal} approach performs statistically significantly better than the expected number of turns for guessing (the strategy for the untrained fold 0 system) for the remainder of the games.
The \textbf{vision only} system, by contrast, is never able to differentiate itself significantly from random guessing, even as more training data becomes available.
We suspect the number of objects is too small for the \textbf{vision only} system to develop decent models of many predicates, whereas \textbf{multi-modal} exploration allows that system to extract more information per object.

\textbf{Human guess.} Neither the \textbf{vision only} nor \textbf{multi-modal} system's performance improves on this metric with statistical significance as more training data is seen.
Human guesses hovered around 2.5 throughout all levels of training and sets of objects.

This result highlights the difficulty of the robot's turn in an \ispy framework, which requires not just good coverage of grounded words (as when figuring out what object the human is describing), but also high accuracy when using classifiers on new objects.
Context classifiers with few examples could achieve confidence $\kappa=1$, making the predicates they represented more likely to be chosen to describe objects.
It is possible that the system would have performed better on this metric if the predicate scoring function $R$ additionally favored predicates with many examples.

\textbf{Predicate Agreement.} Training the predicate classifiers using leave-one-out cross validation over objects, we calculated the average precision, recall, and $F_1$ scores of each against human predicate labels on the held-out object.
Table~\ref{tab:predicate_results} gives these metrics for the 74 predicates used by the systems.\footnote{There were 53 predicates shared between the two systems.
The results in Table~\ref{tab:predicate_results} are similar for a paired $t$-test across these shared predicates with slightly reduced significance.}

\begin{table}
\centering
\begin{tabular}[h]{|l|r|r|}
	\hline
	\bf Metric & \multicolumn{2}{c|}{\bf System} \\ \hline \hline
	& \bf vision only & \bf multi-modal \\ \hline
	precision & .250 & .378\textbf{+} \\
	recall & .179 & .348\textbf{*} \\
	\bf $F_1$ & .196 & .354\textbf{*} \\ \hline
\end{tabular}
\caption{Average performance of predicate classifiers used by the \textbf{vision only} and \textbf{multi-modal} systems in leave-one-object-out cross validation.
\textbf{*}: significantly greater than competing system with $p<0.05$.
\textbf{+}: $p<0.1$ (Student's un-paired $t$-test).}
\label{tab:predicate_results}
\end{table}

Across the objects our robot explored, our \textbf{multi-modal} system achieves consistently better agreement with human assignments of predicates to objects than does the \textbf{vision only} system.
