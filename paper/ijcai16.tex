\typeout{IJCAI-16 Instructions for Authors}

\documentclass{article}
% The file ijcai16.sty is the style file for IJCAI-16 (same as ijcai07.sty).
\usepackage{ijcai16}

% Use the postscript times font!
\usepackage{times}

% the following package is optional:
%\usepackage{latexsym} 

% author added packages
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{array}

\newcommand{\ispy}{``I Spy''\xspace}
\newcommand{\examplepicsize}{0.125}
\newcommand{\pictablew}{0.4in}

%\title{Grounded Attribute Learning with Multi-Modal Perception}
\title{Learning Multi-Modal Grounded Linguistic Semantics by Playing \ispy}
\author{Jesse Thomason, Jivko Sinapov, Maxwell Svetlik, Peter Stone,\and Raymond J. Mooney \\
Department of Computer Science, University of Texas at Austin\\
Austin, TX 78712, USA\\
\{jesse, jsinapov, maxwell, pstone, mooney\}@cs.utexas.edu}

\begin{document}

\maketitle

\begin{abstract}
% The abstract should be no more than 200 words long
Grounded language learning bridges words like `red' and `square' with robot perception.
The vast majority of existing work in this space limits robot perception to vision.
In this paper, we build perceptual models that use haptic, auditory, and proprioceptive data acquired through robot exploratory behaviors to go beyond vision.
Our system learns to ground natural language words describing objects using supervision from an interactive human-robot \ispy game.
In this game, the human and robot take turns describing one object among several, then trying to guess which object the other has described.
All supervision labels were gathered from human participants physically present to play this game with a robot.
We demonstrate that our multi-modal system for grounding natural language outperforms a traditional, vision-only grounding framework by comparing the two on the \ispy task.
We also provide a qualitative analysis of the groundings learned in the game, visualizing what words are understood better with multi-modal sensory information as well as identifying learned word meanings that correlate with physical object properties (e.g. `small' negatively correlates with object weight).
\end{abstract}

\section{Introduction}
\label{sec:introduction}
	\input{introduction.tex}

\section{Related Work}
\label{sec:relatedwork}
	\input{relatedwork.tex}

\section{Dataset}
\label{sec:dataset}
	\input{dataset.tex}

\section{Task Definition}
\label{sec:taskdefinition}
	\input{taskdefinition.tex}

\section{Implementation}
\label{sec:implementation}
	\input{implementation.tex}

	\subsection{Multi-Modal Perception}
	\label{ssec:mmp}
	\input{mmp.tex}

	\subsection{Grounded Language Learning}
	\label{ssec:gll}
	\input{gll.tex}

\section{Experiment}
\label{sec:experiment}
	\input{experiment.tex}

	\subsection{Methodology}
	\label{ssec:methodology}
	\input{methodology.tex}

	\subsection{Quantitative Results}
	\label{ssec:results}
	\input{results.tex}

	\subsection{Qualitative Results}
	\label{ssec:qualitative}
	\input{qualitative.tex}

\section{Conclusion}
\label{sec:conclusion}
\input{conclusion.tex}

\section*{Acknowledgments}

We would like to thank our anonymous reviewers for their feedback and insights, our many participants for their time, and Subhashini Venugopalan for her help in engineering deep visual feature extraction. This work is supported by a National Science Foundation Graduate Research Fellowship to the first author and an NSF EAGER grant (IIS-1548567). A portion of this work has taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (CNS-1330072, CNS-1305287), ONR (21C184-01), and AFOSR (FA8750-14-1-0070, FA9550-14-1-0087).

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{/u/ml/bib/lunar,/u/jesse/local}

\end{document}
