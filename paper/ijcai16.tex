\typeout{IJCAI-16 Instructions for Authors}

\documentclass{article}
% The file ijcai16.sty is the style file for IJCAI-16 (same as ijcai07.sty).
\usepackage{ijcai16}

% Use the postscript times font!
\usepackage{times}

% the following package is optional:
%\usepackage{latexsym} 

\newcommand{\ispy}{\textit{I, Spy} }

\title{Grounded Attribute Learning with Multi-Modal Perception}
\author{Paper XXX}
%\author{name \\ 
%affiliation  \\
%email}

\begin{document}

\maketitle

\begin{abstract}
	% The abstract should be no more than 200 words long
	We build models of object attributes that use visual, haptic, auditory, and proprioceptive perceptual data, acquired through robot exploratory behaviors, using supervision from human-robot interaction.
\end{abstract}

\section{Introduction}

	In order to have meaningful verbal interaction with humans, robots need to be able to connect language to their environment. Mapping from referring expressions like ``the blue cup'' to their object referents in the real world is part of the \textit{language grounding problem}~\cite{that old roy paper i think}.

\section{Related Work}

	% relation to grounded language learning tasks
	In the past few years, researchers have made substantial progress on grounding language for robots, enabling tasks like object recognition and route following from verbal descriptions.

	Much of this work has focused on combining language with visual information. For grounding referring expressions in an environment, many learn perceptual classifiers for words like `red', `square', and `left' given some pairing of human descriptions and labeled scenes~\cite{liu:acl14,malinowski:nips14,mohan:acs13,sun:icra13,dindo:iros10}. Some works additionally incorporate language models into the learning phase~\cite{spranger:ijcai15,krishnamurthy:acl13,perera:aaai13,matuszek:icml12}. Our method uses simple language understanding and constructs new attribute classifiers for new, unseen words used by a human playing \ispy. Unlike any previous methods, our classifiers take advantage of more than just visual data from objects.

	Including a human in the learning loop provides a more realistic learning scenario for applications such as household robotics. Past work has used human speech plus gestures describing sets of objects on a table as supervision to learn attribute classifiers~\cite{matuszek:aaai14,kollar:rss13}. Recent work introduced the \ispy game as a framework for supervision~\cite{parde:ijcai15} for grounded language learning. Outside of robotics, there has been some work on combining language with other sensory modalities than vision, such as audio~\cite{kiela:emnlp15}. Our work differs from these in that we use experience gathered from more than vision to build object attribute classifiers. In our instantiation of the \ispy task, the robot and the human both take a turn describing objects, where in previous work~\shortcite{parde:ijcai15} only the human played this role. To our knowledge, ours is the first work to integrate visual, haptic, auditory, and proprioceptive information for language grounding by an embodied robot.

	% relation to multi-modal perception (i.e. jivko's work)
	% ordinal objects paper for which data was gathered should be accepted Jan 15 and can then be cited

\section{Task Definition}

\section{Implementation}

	\subsection{Grounded Language Learning}

	\subsection{Multi-Modal Perception}

\section{Experiment}

	\subsection{Setup}

	\subsection{Results}

% \section*{Acknowledgments}

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\newpage
\bibliographystyle{named}
\bibliography{/u/ml/bib/lunar,/u/jesse/local}

\end{document}
