\typeout{IJCAI-16 Instructions for Authors}

\documentclass{article}
% The file ijcai16.sty is the style file for IJCAI-16 (same as ijcai07.sty).
\usepackage{ijcai16}

% Use the postscript times font!
\usepackage{times}

% the following package is optional:
%\usepackage{latexsym} 

% author added packages
\usepackage{xspace}

\newcommand{\ispy}{\textit{I, Spy}\xspace}

\title{Grounded Attribute Learning with Multi-Modal Perception}
%\title{Learning Multi-Modal Grounded Linguistic Semantics by Playing \ispy}
\author{Paper XXX}
%\author{name \\ 
%affiliation  \\
%email}

\begin{document}

\maketitle

\begin{abstract}
% The abstract should be no more than 200 words long
Grounded language learning bridges human language words like `red' and `square' with robot perception.
The vast majority of existing work in this space limits perception to robot vision.
In this paper, we build perceptual models that use haptic, auditory, and proprioceptive data acquired through robot exploratory behaviors that go beyond vision alone.
Our system learns to ground natural language predicates about objects using supervision from an interactive human-robot \ispy game.
In this game, the human and robot take turns describing an object on a table, then trying to guess which object the other has described.
All predicate labels were gathered from human subjects physically present to play this game with the robot, which conducted a dialog to run the game, pointed to objects to guess them, and detected human touches of objects as guesses.
We demonstrate that our multi-modal system for grounding natural language outperforms a traditional, vision-only grounding framework by comparing the two on the \ispy task.
We also provide a qualitative analysis of the predicates learned in the game, visualizing groups of predicates that can be understood from the same kinds of robot perception and pointing out predicates for which vision alone is insufficient (e.g. `heavy').
\end{abstract}

\section{Introduction}
\label{sec:introduction}
	\input{introduction.tex}

\section{Related Work}
\label{sec:relatedwork}
	\input{relatedwork.tex}

\section{Task Definition}
\label{sec:taskdefinition}
	\input{taskdefinition.tex}

\section{Object Dataset}
\label{sec:dataset}
	\input{dataset.tex}

\section{Implementation}
\label{sec:implementation}
	\input{implementation.tex}

	\subsection{Multi-Modal Perception}
	\label{ssec:mmp}
	\input{mmp.tex}

	\subsection{Grounded Language Learning}
	\label{ssec:gll}
	\input{gll.tex}

\section{Experiment}
\label{sec:experiment}
	\input{experiment.tex}

	\subsection{Methodology}
	\label{ssec:methodology}
	\input{methodology.tex}

	\subsection{Results}
	\label{ssec:results}

		% average robot turns to guess human pick over folds; average human turns to guess robot pick over folds; statistical comparison to true random (0.25), maybe fold-fold tests too
		% if enough users return, can do paired tests of fold0-fold3 for example

\section{Conclusion}
\label{sec:conclusion}

% \section*{Acknowledgments}

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\newpage
\bibliographystyle{named}
\bibliography{/u/ml/bib/lunar,/u/jesse/local}

\end{document}
