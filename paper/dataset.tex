\begin{table}
\centering
\begin{tabular}[h]{|l|r|r|r|}
	\hline
	\bf Behavior & \multicolumn{3}{c|}{\bf Modality} \\ \hline \hline
	& \bf color & \bf fpfh & \bf vgg \\ \hline
	\bf look & 64 & 308 & 4096 \\ \hline \hline
	& \bf audio & \bf haptics & \bf finger \\ \hline
	\bf grasp & 100 & 60 & 20 \\ \hline
	\bf drop, hold & & & \\
	\bf lift, lower & 100 & 60 & \\
	\bf press, push & & & \\ \hline
\end{tabular}
\caption{The number of features extracted from each \textit{context}, or combination of robot behavior and perceptual modality.}
\label{tab:feature_space_of_contexts}
\end{table}

In order to make use of the raw data collected, we choose a feature representation and downsample for each modality.
Table~\ref{tab:feature_space_of_contexts} gives the size of the feature vectors extracted for each \textit{context}, or combination of robot exploratory behavior and perceptual modality.

Using a 3D pointcloud, we define the object's local invarient features using the Fast Point Feature Histogram~\cite{rusu:icra09}  across 308 bins (\textbf{fpfh}), and gather the color distribution in a histogram into 64 bins (\textbf{color}).
Using RGB images of objects, we extract deep visual features from the 16-layer VGG network~\cite{simonyan:corr14} (\textbf{vgg}).

Audio features are described by the Discrete Fourier Transform of the recorded audio sample, and then downsampled into 10 time and 10 frequency bins (\textbf{audio}). 
Haptic data is recorded for each of the 6 joints of the arm and are represented as raw features, reduced into 10 time bins for each joint (\textbf{audio}).

Finally, proprioception is represented by the finger locations on the arm's end effector as a range from 0 to 1500, which corresponds to the fingers being closed and open respectively. 
Proprioceptive data is recorded only for the \textbf{grasp} behavior and thus has a uniform length of 20 time bins (\textbf{finger}). 