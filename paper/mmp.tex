For each language predicate $p$, a classifier $D_p$ was learned to decide whether objects possessed the attribute denoted by $p$.
This classifier was informed by sub-classifiers that determined whether $p$ held for a particular subset of the features used to describe objects.

The feature space of objects was partitioned according contexts, as discussed in Section~\ref{ssec:contexts}.
Each context classifier $M_{c}, c\in\mathcal{C}$ is a quadratic-kernel SVM trained with positive- and negative-labels for context feature vectors derived from the \ispy game (Section~\ref{ssec:gll}).
Then $M_{c}(\mathcal{X}_i^c)\in [-1,1]$, the average classifier output over all observations for object $i\in\mathcal{O}$ where individual SVM decisions on observations were in $\{-1,1\}$.
Following previous work in multi-modal exploration~\cite{sinapov:icra14}, for each context we calculated the Cohen's Kappa $\kappa_{c}$ to measure the agreement across observations between the decisions of the $M_{c}$ classifier and the ground truth labels from the \ispy game, in order to serve as a confidence in $[0,1]$ for each context
\footnote{We use $\kappa$ instead of accuracy because we can expect most objects to be negative examples of a given predicate. $\kappa$ better handles such skewed-class data than accuracy, which could be deceptively high for a classifier that always returns false.}
We ceiling negative $\kappa$ to $0$.

Given these context classifiers and associated confidences, we calculate an overall decision confidence, $D_p(i)$, for $i\in\mathcal{O}$ for each behavior $b$ and modality $m$ as:
\begin{equation}
	D_p(i) = \sum_{c\in\mathcal{C}}{\kappa_{c} M_{c}(\mathcal{X}_i^c)} \in [-1,1]
\end{equation}
The sign of $D_p(i)$ gives a decision on whether $p$ applies to $i$ with confidence $|D_p(i)|$.
